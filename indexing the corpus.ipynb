{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "232eb573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n",
      "cluster-6311  GCE       4                                       RUNNING  us-central1-a\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if the following command generates an error, you probably didn't enable \n",
    "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
    "# under Manage Security → Project Access when setting up the cluster\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16654f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a66955bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942b99b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 247882 Jan  9 10:30 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if nothing prints here you forgot to include the initialization script when starting the cluster\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5e8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9746f0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-6311-m.c.stable-apogee-366911.internal:45297\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f904f161cd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fc52c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Put your bucket name below and make sure you can access it without an error\n",
    "bucket_name = '205915135'\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths=[]\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if b.name != 'graphframes.sh':\n",
    "        paths.append(full_path+b.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2c62f",
   "metadata": {},
   "source": [
    "Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c88dbefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverted_index_gcp.py\r\n"
     ]
    }
   ],
   "source": [
    "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b94b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# adding our python module to the cluster\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0,SparkFiles.getRootDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b085fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inverted_index_gcp import *\n",
    "TUPLE_SIZE = 6\n",
    "TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n",
    "from contextlib import closing\n",
    "\n",
    "def read_posting_list(inverted, w,file_name):\n",
    "  with closing(MultiFileReader()) as reader:\n",
    "    locs = inverted.posting_locs[w]\n",
    "    locs = [(file_name + lo[0], lo[1]) for lo in locs]\n",
    "    b = reader.read(locs, inverted.df[w] * TUPLE_SIZE)\n",
    "    posting_list = []\n",
    "    for i in range(inverted.df[w]):\n",
    "      doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n",
    "      tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n",
    "      posting_list.append((doc_id, tf))\n",
    "    return posting_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "410865fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "def token2bucket_id(token):\n",
    "  return int(_hash(token),16) % NUM_BUCKETS\n",
    "\n",
    "# PLACE YOUR CODE HERE\n",
    "def word_count(text, id):\n",
    "  ''' Count the frequency of each word in `text` (tf) that is not included in \n",
    "  `all_stopwords` and return entries that will go into our posting lists. \n",
    "  Parameters:\n",
    "  -----------\n",
    "    text: str\n",
    "      Text of one document\n",
    "    id: int\n",
    "      Document id\n",
    "  Returns:\n",
    "  --------\n",
    "    List of tuples\n",
    "      A list of (token, (doc_id, tf)) pairs \n",
    "      for example: [(\"Anarchism\", (12, 5)), ...]\n",
    "  '''\n",
    "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "  # YOUR CODE HERE\n",
    "#   raise NotImplementedError()\n",
    "  tf=Counter(tokens)\n",
    "  return [(token, (id, tf[token])) for token in tf if token not in all_stopwords]\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "  ''' Returns a sorted posting list by wiki_id.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    unsorted_pl: list of tuples\n",
    "      A list of (wiki_id, tf) tuples \n",
    "  Returns:\n",
    "  --------\n",
    "    list of tuples\n",
    "      A sorted posting list.\n",
    "  '''\n",
    "  # YOUR CODE HERE\n",
    "  #raise NotImplementedError()\n",
    "  return sorted(unsorted_pl, key=lambda x: x[0])\n",
    "def calculate_df(postings):\n",
    "  ''' Takes a posting list RDD and calculate the df for each token.\n",
    "  Parameters:\n",
    "  -----------\n",
    "    postings: RDD\n",
    "      An RDD where each element is a (token, posting_list) pair.\n",
    "  Returns:\n",
    "  --------\n",
    "    RDD\n",
    "      An RDD where each element is a (token, df) pair.\n",
    "  '''\n",
    "  # YOUR CODE HERE\n",
    "  #raise NotImplementedError()\n",
    "  return postings.map(lambda x: (x[0], len(x[1])))\n",
    "  \n",
    "def partition_postings_and_write(postings ):\n",
    "  ''' A function that partitions the posting lists into buckets, writes out \n",
    "  all posting lists in a bucket to disk, and returns the posting locations for \n",
    "  each bucket. Partitioning should be done through the use of `token2bucket` \n",
    "  above. Writing to disk should use the function  `write_a_posting_list`, a \n",
    "  static method implemented in inverted_index_colab.py under the InvertedIndex \n",
    "  class. \n",
    "  Parameters:\n",
    "  -----------\n",
    "    postings: RDD\n",
    "      An RDD where each item is a (w, posting_list) pair.\n",
    "  Returns:\n",
    "  --------\n",
    "    RDD\n",
    "      An RDD where each item is a posting locations dictionary for a bucket. The\n",
    "      posting locations maintain a list for each word of file locations and \n",
    "      offsets its posting list was written to. See `write_a_posting_list` for \n",
    "      more details.\n",
    "  '''\n",
    "  # YOUR CODE HERE\n",
    "  #raise NotImplementedError()\n",
    "  # partition the posting lists into buckets\n",
    "  postings = postings.map(lambda x: (token2bucket_id(x[0]), x))\n",
    "  # write out all posting lists in a bucket to disk\n",
    "  postings = postings.groupByKey()\n",
    "  postings = postings.map(lambda x: InvertedIndex.write_a_posting_list(x,bucket_name))\n",
    "  # return the posting locations for each bucket\n",
    "  return postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "976c2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "\n",
    "def calculate_term_total(postings):\n",
    "    ''' Takes a posting list RDD and calculate the term total for each token.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    postings: RDD\n",
    "        An RDD where each element is a (token, posting_list) pair.\n",
    "    Returns:\n",
    "    --------\n",
    "    RDD\n",
    "        An RDD where each element is a (token, term_total) pair.\n",
    "    '''\n",
    "    return postings.mapValues(lambda posting: builtins.sum([tf for doc_id, tf in posting]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d503645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.shell import spark\n",
    "\n",
    "parquetFile = spark.read.parquet(*paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b84aa867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6348910"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquetFile.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771ba2b5",
   "metadata": {},
   "source": [
    "## Title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dee472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_text_pairs_title = parquetFile.select(\"title\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "173a4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#title\n",
    "word_counts_title = doc_text_pairs_title.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "postings_title = word_counts_title.groupByKey().mapValues(reduce_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8c95dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "w2df_title = calculate_df(postings_title)\n",
    "w2df_dict = w2df_title.collectAsMap()\n",
    "\n",
    "w2terms = calculate_term_total(postings_title)\n",
    "dict_term_total = w2terms.collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd533e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# partition posting lists and write out\n",
    "_ = partition_postings_and_write(postings_title).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c30429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all posting lists locations into one super-set\n",
    "super_posting_locs_title = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs_title[k].extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65bd1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inverted index instance\n",
    "inverted_title = InvertedIndex()\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_title.posting_locs = super_posting_locs_title\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted_title.df = w2df_dict\n",
    "# Add the total terms\n",
    "inverted_title.term_total=dict_term_total\n",
    "# write the global stats out\n",
    "inverted_title.write_index('.', 'title_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9c8353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_title=pd.read_pickle('/home/dataproc/title_index.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26aa4ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1774265"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_title.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ed2160",
   "metadata": {},
   "source": [
    "## BODY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "079a2ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize import tokenize\n",
    "\n",
    "\n",
    "def doc_len(id, text):\n",
    "    ''' Calculate the length of the text\n",
    "    Parameters:\n",
    "    -----------\n",
    "    id: int\n",
    "        Document id\n",
    "    text: str\n",
    "        Text of one document\n",
    "    Returns:\n",
    "    --------\n",
    "    A dict of (doc_id, doc_len) pairs \n",
    "    for example: {12:5, ...}\n",
    "    '''\n",
    "    tokenized = tokenize(text)\n",
    "    result = []\n",
    "    return [(id, len(tokenized))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1fdf301",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_text_pairs_body = parquetFile.select(\"text\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2998210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#body\n",
    "word_counts_of_body = doc_text_pairs_body.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "postings_body = word_counts_of_body.groupByKey().mapValues(reduce_word_counts)\n",
    "#doc lengh\n",
    "dl=doc_text_pairs_body.flatMap(lambda x: doc_len(x[1], x[0])) \n",
    "postings_filtered_body = postings_body.filter(lambda x: len(x[1])>50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc15e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "w2df_body = calculate_df(postings_filtered_body)\n",
    "w2df_body = w2df_body.collectAsMap()\n",
    "\n",
    "w2termsbody = calculate_term_total(postings_filtered_body)\n",
    "dict_term_total_body = w2termsbody.collectAsMap()\n",
    "\n",
    "\n",
    "_ = partition_postings_and_write(postings_filtered_body).collect()\n",
    "# collect all posting lists locations into one super-set\n",
    "super_posting_locs_body = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs_body[k].extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74d7fba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/10 02:04:23 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 25.0 (TID 1834) (cluster-6311-w-1.c.stable-apogee-366911.internal executor 34): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_8953/4169666495.py\", line 5, in <lambda>\n",
      "  File \"/tmp/ipykernel_8953/47135752.py\", line 14, in doc_len\n",
      "NameError: name 'tokenize' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/01/10 02:04:34 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 9.0 in stage 25.0 (TID 1844) (cluster-6311-w-1.c.stable-apogee-366911.internal executor 34): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_8953/4169666495.py\", line 5, in <lambda>\n",
      "  File \"/tmp/ipykernel_8953/47135752.py\", line 14, in doc_len\n",
      "NameError: name 'tokenize' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/01/10 02:04:45 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 9.3 in stage 25.0 (TID 1854) (cluster-6311-w-1.c.stable-apogee-366911.internal executor 34): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_8953/4169666495.py\", line 5, in <lambda>\n",
      "  File \"/tmp/ipykernel_8953/47135752.py\", line 14, in doc_len\n",
      "NameError: name 'tokenize' is not defined\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/01/10 02:04:45 ERROR org.apache.spark.scheduler.TaskSetManager: Task 9 in stage 25.0 failed 4 times; aborting job\n",
      "23/01/10 02:04:45 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 16.0 in stage 25.0 (TID 1858) (cluster-6311-w-1.c.stable-apogee-366911.internal executor 34): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 25.0 failed 4 times, most recent failure: Lost task 9.3 in stage 25.0 (TID 1854) (cluster-6311-w-1.c.stable-apogee-366911.internal executor 34): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_8953/4169666495.py\", line 5, in <lambda>\n  File \"/tmp/ipykernel_8953/47135752.py\", line 14, in doc_len\nNameError: name 'tokenize' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_8953/4169666495.py\", line 5, in <lambda>\n  File \"/tmp/ipykernel_8953/47135752.py\", line 14, in doc_len\nNameError: name 'tokenize' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m inverted_body\u001B[38;5;241m.\u001B[39mterm_total \u001B[38;5;241m=\u001B[39m dict_term_total_body\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Add the doc lenth\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m inverted_body\u001B[38;5;241m.\u001B[39mDL\u001B[38;5;241m=\u001B[39m\u001B[43mdl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectAsMap\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# write the global stats out\u001B[39;00m\n\u001B[1;32m     11\u001B[0m inverted_body\u001B[38;5;241m.\u001B[39mwrite_index(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody_index\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py:1847\u001B[0m, in \u001B[0;36mRDD.collectAsMap\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1830\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcollectAsMap\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   1831\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1832\u001B[0m \u001B[38;5;124;03m    Return the key-value pairs in this RDD to the master as a dictionary.\u001B[39;00m\n\u001B[1;32m   1833\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1845\u001B[0m \u001B[38;5;124;03m    4\u001B[39;00m\n\u001B[1;32m   1846\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1847\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py:949\u001B[0m, in \u001B[0;36mRDD.collect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    940\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    941\u001B[0m \u001B[38;5;124;03mReturn a list that contains all of the elements in this RDD.\u001B[39;00m\n\u001B[1;32m    942\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    946\u001B[0m \u001B[38;5;124;03mto be small, as all the data is loaded into the driver's memory.\u001B[39;00m\n\u001B[1;32m    947\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    948\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext) \u001B[38;5;28;01mas\u001B[39;00m css:\n\u001B[0;32m--> 949\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonRDD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectAndServe\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrdd\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    950\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jrdd_deserializer))\n",
      "File \u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1298\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1299\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1300\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1301\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1303\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1304\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1305\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1307\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1308\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:111\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw):\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 111\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m py4j\u001B[38;5;241m.\u001B[39mprotocol\u001B[38;5;241m.\u001B[39mPy4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    113\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 25.0 failed 4 times, most recent failure: Lost task 9.3 in stage 25.0 (TID 1854) (cluster-6311-w-1.c.stable-apogee-366911.internal executor 34): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_8953/4169666495.py\", line 5, in <lambda>\n  File \"/tmp/ipykernel_8953/47135752.py\", line 14, in doc_len\nNameError: name 'tokenize' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_8953/4169666495.py\", line 5, in <lambda>\n  File \"/tmp/ipykernel_8953/47135752.py\", line 14, in doc_len\nNameError: name 'tokenize' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/10 02:04:46 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 12.2 in stage 25.0 (TID 1852) (cluster-6311-w-3.c.stable-apogee-366911.internal executor 39): TaskKilled (Stage cancelled)\n",
      "23/01/10 02:04:46 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 10.0 in stage 25.0 (TID 1846) (cluster-6311-w-1.c.stable-apogee-366911.internal executor 42): TaskKilled (Stage cancelled)\n",
      "23/01/10 02:04:46 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 5.0 in stage 25.0 (TID 1839) (cluster-6311-w-3.c.stable-apogee-366911.internal executor 38): TaskKilled (Stage cancelled)\n",
      "23/01/10 02:04:47 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 7.1 in stage 25.0 (TID 1845) (cluster-6311-w-2.c.stable-apogee-366911.internal executor 43): TaskKilled (Stage cancelled)\n",
      "23/01/10 02:04:47 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 13.1 in stage 25.0 (TID 1855) (cluster-6311-w-0.c.stable-apogee-366911.internal executor 40): TaskKilled (Stage cancelled)\n",
      "23/01/10 02:04:47 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 15.0 in stage 25.0 (TID 1857) (cluster-6311-w-2.c.stable-apogee-366911.internal executor 44): TaskKilled (Stage cancelled)\n",
      "23/01/10 02:04:48 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.3 in stage 25.0 (TID 1842) (cluster-6311-w-0.c.stable-apogee-366911.internal executor 41): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "inverted_body = InvertedIndex()\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_body.posting_locs = super_posting_locs_body\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted_body.df = w2df_body\n",
    "#Add the total terms\n",
    "inverted_body.term_total = dict_term_total_body\n",
    "# Add the doc lenth\n",
    "inverted_body.DL=dl.collectAsMap()\n",
    "# write the global stats out\n",
    "inverted_body.write_index('.','body_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d31ee7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'theoretical': 34848,\n",
       " 'ahmadu': 894,\n",
       " 'trawler': 2341,\n",
       " 'omagbemi': 76,\n",
       " 'frankokratia': 55,\n",
       " 'gooyer': 74,\n",
       " 'nuclear': 57151,\n",
       " 'bloody': 22010,\n",
       " 'mckinnon': 2908,\n",
       " 'distributional': 697,\n",
       " 'plantas': 271,\n",
       " 'compsognathus': 101,\n",
       " 'film2006': 559,\n",
       " 'sesamoid': 160,\n",
       " '5james': 88,\n",
       " 'baltz': 194,\n",
       " 'kg60': 163,\n",
       " 'helvetica': 553,\n",
       " 'balandin': 154,\n",
       " 'etem': 87,\n",
       " 'tatem': 87,\n",
       " 'mannen': 427,\n",
       " 'sureness': 120,\n",
       " 'vällingby': 85,\n",
       " 'daham': 147,\n",
       " 'per-erik': 107,\n",
       " 'sharavathi': 65,\n",
       " 'bassman': 334,\n",
       " 'kehot': 63,\n",
       " 'akros': 73,\n",
       " 'tarma': 199,\n",
       " 'nimir': 74,\n",
       " 'devanāgarī': 113,\n",
       " 'theretofore': 142,\n",
       " \"parade's\": 383,\n",
       " 'pin': 16223,\n",
       " 'biaggi': 590,\n",
       " 'sefolosha': 76,\n",
       " 'labora': 106,\n",
       " 'pridgen': 85,\n",
       " '33-17': 78,\n",
       " 'skratch': 164,\n",
       " 'maramureş': 117,\n",
       " 'cxt': 72,\n",
       " 'dzama': 67,\n",
       " '5834': 93,\n",
       " \"subchannel's\": 59,\n",
       " 'batenburg': 51,\n",
       " 'slatter': 226,\n",
       " 'pendulus': 77,\n",
       " '9892': 69,\n",
       " 'warlick': 144,\n",
       " 'mandakini': 314,\n",
       " 'an-nasir': 277,\n",
       " 'streptococcaceae': 53,\n",
       " '1963-1972': 77,\n",
       " \"okamoto's\": 94,\n",
       " \"usaf's\": 395,\n",
       " 'al-rifai': 100,\n",
       " 'multiversity': 158,\n",
       " 'sentis': 59,\n",
       " 'puppeteering': 170,\n",
       " 'thwaitesii': 109,\n",
       " 'aurilia': 66,\n",
       " 'corraface': 56,\n",
       " 'drukarnia': 53,\n",
       " 'lange-nielsen': 52,\n",
       " 'karup': 121,\n",
       " 'torpegaard': 56,\n",
       " 'morfa': 250,\n",
       " 'tri-cities': 1366,\n",
       " 'hart-davis': 299,\n",
       " 'alamosa': 568,\n",
       " 'pgb': 77,\n",
       " 'b42': 120,\n",
       " 'kollegium': 108,\n",
       " 'cambios': 73,\n",
       " 'betsi': 112,\n",
       " 'luyten': 141,\n",
       " 'flushwork': 55,\n",
       " 'grennell': 68,\n",
       " 'silver': 204158,\n",
       " 'd69': 78,\n",
       " 'kos': 2853,\n",
       " 'doreen': 3213,\n",
       " 'kov': 110,\n",
       " 'zagora': 1522,\n",
       " 'thrilled': 3633,\n",
       " 'servitude': 2918,\n",
       " 'storleer': 199,\n",
       " 'd-co': 141,\n",
       " 'woodfield': 474,\n",
       " 'watermark': 1176,\n",
       " 'closed-cycle': 73,\n",
       " 'dynamos': 1458,\n",
       " 'tiefe': 154,\n",
       " \"yung's\": 101,\n",
       " 'jidaigeki': 643,\n",
       " 'white-faced': 854,\n",
       " 'mytton': 215,\n",
       " 'xna': 166,\n",
       " 'igg1': 131,\n",
       " '18-19': 782,\n",
       " '1909-1994': 52,\n",
       " 'brahmas': 195,\n",
       " 'am-fm': 294,\n",
       " 'ensnared': 459,\n",
       " 'kersaint': 85,\n",
       " \"transistor's\": 66,\n",
       " 'sanbi': 115,\n",
       " 'padley': 165,\n",
       " 'field-programmable': 204,\n",
       " 'gossard': 353,\n",
       " 'strobl': 749,\n",
       " 'corn-based': 136,\n",
       " 'woo-hee': 90,\n",
       " '8481': 127,\n",
       " \"um's\": 103,\n",
       " 'kazakov': 425,\n",
       " 'magat': 175,\n",
       " 'manten': 60,\n",
       " 'nabs': 140,\n",
       " 'tysons': 271,\n",
       " 'craziest': 489,\n",
       " 'biggert': 74,\n",
       " 'gub': 167,\n",
       " 'recompensed': 112,\n",
       " 'flesch': 469,\n",
       " 'springhill': 692,\n",
       " 'dropout': 2095,\n",
       " 'providentia': 100,\n",
       " 'marjoe': 88,\n",
       " 'vengaboys': 166,\n",
       " 'perjurers': 116,\n",
       " 'akoya': 68,\n",
       " 'bloomsday': 178,\n",
       " 'bhadravati': 67,\n",
       " 'coolly': 661,\n",
       " 'ubriaco': 60,\n",
       " 'double-bladed': 168,\n",
       " 'leonelli': 68,\n",
       " 'marylouise': 62,\n",
       " 'johl': 66,\n",
       " 'anti-realist': 80,\n",
       " '20fc': 69,\n",
       " 'fromfeedatesource15tbd': 58,\n",
       " 'gentler': 1226,\n",
       " 'stadionul': 615,\n",
       " 'giu': 84,\n",
       " 'druckenmiller': 118,\n",
       " 'ratmir': 108,\n",
       " \"clash's\": 271,\n",
       " 'koskoff': 89,\n",
       " '19922': 171,\n",
       " 'nähe': 95,\n",
       " 'vindobonensis': 152,\n",
       " 'homunculi': 80,\n",
       " 'eson': 61,\n",
       " 'fahne': 185,\n",
       " 'muftah': 114,\n",
       " 'cornerman': 242,\n",
       " 'dunalley': 69,\n",
       " 'zia': 2769,\n",
       " 'shaver': 1258,\n",
       " 'boskovice': 99,\n",
       " 'cappie': 143,\n",
       " 'santista': 179,\n",
       " 'willmar': 382,\n",
       " 'reciprocation': 244,\n",
       " 'ropework': 111,\n",
       " 'palatul': 64,\n",
       " 'miseries': 631,\n",
       " 'cn-235': 153,\n",
       " '4951': 164,\n",
       " 'brevik': 233,\n",
       " 'popelka': 52,\n",
       " 'd70': 128,\n",
       " 'kondi': 101,\n",
       " 'mcshorts': 73,\n",
       " 'oryzias': 78,\n",
       " 'bowrey': 207,\n",
       " 'hard-earned': 340,\n",
       " 'punda': 78,\n",
       " 'newcourt': 65,\n",
       " 'herner': 58,\n",
       " 'aghadoe': 190,\n",
       " 'oski': 62,\n",
       " 'fralin': 74,\n",
       " 'falam': 106,\n",
       " 'xiz': 200,\n",
       " 'yearclassvehiclepositions': 51,\n",
       " 'datan': 63,\n",
       " '10076': 78,\n",
       " 'downspouts': 76,\n",
       " 'roback': 125,\n",
       " 'krathong': 76,\n",
       " '0572': 92,\n",
       " 'yaad': 697,\n",
       " 'semester-long': 210,\n",
       " 'bdnf': 227,\n",
       " 'éramos': 94,\n",
       " 'deepa': 1787,\n",
       " 'roh': 1988,\n",
       " 'monoid': 376,\n",
       " 'newsweeklies': 124,\n",
       " 'incirlik': 293,\n",
       " 'crede': 148,\n",
       " 'murfin': 184,\n",
       " 'leiba': 83,\n",
       " 'barkston': 196,\n",
       " 'hpf': 91,\n",
       " 'lithotripsy': 97,\n",
       " \"brodmann's\": 57,\n",
       " 'cypris': 76,\n",
       " 'asiamah': 60,\n",
       " 'pole-carew': 53,\n",
       " 'imeretian': 114,\n",
       " 'entomobryidae': 147,\n",
       " 'solemdal': 57,\n",
       " 'meinesz': 57,\n",
       " 'petebuttigieg': 74,\n",
       " 'loges': 283,\n",
       " 'roydon': 248,\n",
       " 'torrei': 78,\n",
       " 'hiromu': 356,\n",
       " 'hinchman': 182,\n",
       " 'maniple': 97,\n",
       " 'badge': 18980,\n",
       " 'tillery': 303,\n",
       " '1991-2011': 65,\n",
       " 'gibt': 431,\n",
       " 'city14': 135,\n",
       " '2008-present': 298,\n",
       " 'fufa': 98,\n",
       " 'raviraj': 112,\n",
       " 'rumia': 143,\n",
       " '30-round': 244,\n",
       " 'qatil': 179,\n",
       " 'llanquihue': 174,\n",
       " \"rigg's\": 66,\n",
       " \"reality's\": 236,\n",
       " \"wilma's\": 74,\n",
       " 'longicauda': 543,\n",
       " 'illyria': 981,\n",
       " 'rubdown': 81,\n",
       " 'creadores': 127,\n",
       " 'ensae': 64,\n",
       " 'kobelev': 67,\n",
       " 'depreciates': 74,\n",
       " 'colubris': 123,\n",
       " 'akutagawa': 507,\n",
       " 'cullimore': 121,\n",
       " 'slawomir': 200,\n",
       " 'riotta': 61,\n",
       " 'potohar': 87,\n",
       " 'collum': 338,\n",
       " 'greenaway': 1310,\n",
       " 'tuck': 4942,\n",
       " 'elze': 103,\n",
       " 'non-protein': 212,\n",
       " 'onufry': 67,\n",
       " 'balmore': 56,\n",
       " 'evaristo': 1144,\n",
       " 'stuart-wortley': 139,\n",
       " 'demographically': 690,\n",
       " 'taxco': 196,\n",
       " 'streptanthus': 59,\n",
       " \"issue's\": 507,\n",
       " \"2014's\": 1029,\n",
       " 'unitarian-universalist': 51,\n",
       " 'remarquable': 72,\n",
       " 'congresbury': 92,\n",
       " 'bizaki': 81,\n",
       " 'turds': 55,\n",
       " 'plowshares': 199,\n",
       " 'bunratty': 129,\n",
       " 'škp': 99,\n",
       " 'bouet': 246,\n",
       " 'jabbeke': 51,\n",
       " '172005': 58,\n",
       " 'physalaemus': 64,\n",
       " 'anthropologie': 481,\n",
       " 'walvis': 618,\n",
       " 'theda': 538,\n",
       " 'talluri': 52,\n",
       " 'cbh': 139,\n",
       " 'lanerossi': 79,\n",
       " 'faszination': 94,\n",
       " 'east': 637172,\n",
       " 'rejecting': 9961,\n",
       " 'tracy': 19953,\n",
       " 'agn': 776,\n",
       " 'chubanshe': 251,\n",
       " 'thuggish': 283,\n",
       " 'calabria': 5254,\n",
       " 'ivisionappsgoalsappsgoals': 4528,\n",
       " 'troyes': 2575,\n",
       " 'ximenez': 97,\n",
       " 'hedegaard': 163,\n",
       " 'hallquist': 72,\n",
       " 'quisiera': 260,\n",
       " 'kajikawa': 68,\n",
       " 'tamgha-i-imtiaz': 64,\n",
       " 'anti-english': 122,\n",
       " 'meave': 70,\n",
       " 'aviat': 77,\n",
       " 'lembi': 61,\n",
       " 'dayle': 243,\n",
       " 'atavus': 102,\n",
       " 'paleozoic': 2905,\n",
       " 'gillick': 448,\n",
       " 'productor': 90,\n",
       " 'hongchen': 55,\n",
       " 'trevino': 942,\n",
       " 'audiobooks': 1785,\n",
       " \"larkin's\": 759,\n",
       " 'hydro-pneumatic': 182,\n",
       " 'uncategorized': 171,\n",
       " 'beaverlodge': 90,\n",
       " 'percept': 182,\n",
       " 'ōkura': 122,\n",
       " 'trika': 66,\n",
       " 'asensio': 340,\n",
       " 'mottama': 115,\n",
       " 'broods': 1274,\n",
       " 'contradictory': 6416,\n",
       " 'gasparini': 508,\n",
       " \"dobson's\": 334,\n",
       " 'energetik': 135,\n",
       " '1904-1906': 161,\n",
       " 'nadigan': 56,\n",
       " 'ochrocephala': 61,\n",
       " 'flowerpiercer': 61,\n",
       " 'bluebelles': 55,\n",
       " 'cover-art': 51,\n",
       " 'bapaiah': 61,\n",
       " 'deep-frying': 117,\n",
       " 'jerfea': 63,\n",
       " 'teampwdltries': 98,\n",
       " 'fumigating': 51,\n",
       " 'pseudomembranous': 67,\n",
       " 'imtech': 58,\n",
       " 'mojtahed': 52,\n",
       " 'behrman': 455,\n",
       " 'bluetail': 141,\n",
       " 'petnjica': 69,\n",
       " 'craske': 111,\n",
       " 'co-organizes': 68,\n",
       " 'coronets': 349,\n",
       " 'fabergé': 472,\n",
       " 'regenerate': 2826,\n",
       " 'viacomcbs': 796,\n",
       " 'darpa': 1611,\n",
       " 'bnei': 2458,\n",
       " 'katarzyna': 2645,\n",
       " 'symbiote': 408,\n",
       " 'aashto': 418,\n",
       " 'acaso': 88,\n",
       " 'turchin': 240,\n",
       " 'hirate': 162,\n",
       " 'crankpins': 60,\n",
       " 'berléand': 116,\n",
       " 'big-bang': 73,\n",
       " 'clapboard-sided': 101,\n",
       " '1949-1954': 140,\n",
       " 'grodzisko': 79,\n",
       " 'waimate': 316,\n",
       " 'abyek': 129,\n",
       " 'feijoa': 53,\n",
       " 'simmone': 109,\n",
       " 'tualang': 71,\n",
       " 'marval': 143,\n",
       " \"1893's\": 51,\n",
       " 'fuyumi': 192,\n",
       " 'yula': 136,\n",
       " 'm-high': 148,\n",
       " 'wbts-cd': 53,\n",
       " 'jacinda': 670,\n",
       " 'diffract': 110,\n",
       " 'st-gelais': 78,\n",
       " 'i-87': 186,\n",
       " 'qualify2016': 51,\n",
       " 'incoherent': 1835,\n",
       " 'rollingstone': 187,\n",
       " 'j12': 121,\n",
       " 'liebler': 69,\n",
       " 'ptw': 73,\n",
       " 'kayes': 582,\n",
       " 'corsham': 437,\n",
       " 'gigantopithecus': 72,\n",
       " 'radel': 85,\n",
       " \"wnyc's\": 127,\n",
       " 'venise': 655,\n",
       " 'permeating': 324,\n",
       " 'stussy': 81,\n",
       " 'pitchside': 65,\n",
       " \"pickford's\": 122,\n",
       " 'ill-natured': 57,\n",
       " 'susheela2': 70,\n",
       " 'fruit-tree': 57,\n",
       " 'teampldwdlbpfpapdpts116x1': 52,\n",
       " \"zidane's\": 70,\n",
       " 'ls6': 64,\n",
       " 'gwangjong': 75,\n",
       " 'mandt': 60,\n",
       " 'pynson': 62,\n",
       " 'brazil25': 56,\n",
       " 'lance': 20497,\n",
       " 'punish': 8652,\n",
       " 'apoptosis-inducing': 68,\n",
       " 'byington': 472,\n",
       " 'utti': 72,\n",
       " 'jhoola': 63,\n",
       " 'summers': 28954,\n",
       " 'terminate': 11056,\n",
       " 'repackaged': 1996,\n",
       " 'pavol': 1131,\n",
       " 'zwaan': 277,\n",
       " 'krantz': 990,\n",
       " 'peletier': 372,\n",
       " 'dreamz': 216,\n",
       " 'ilhan': 446,\n",
       " 'breaching': 3408,\n",
       " 'wayfaring': 336,\n",
       " 'moure': 303,\n",
       " 'adores': 877,\n",
       " '480th': 161,\n",
       " 'quartus': 206,\n",
       " 'swinkels': 95,\n",
       " 'credentialing': 529,\n",
       " 'yutang': 230,\n",
       " 'bna': 789,\n",
       " 'giantslalom': 294,\n",
       " 'shinrikyo': 217,\n",
       " 'wasurenai': 101,\n",
       " '2008west': 56,\n",
       " 'rsc-3': 103,\n",
       " 'seven-acre': 118,\n",
       " \"samantha's\": 354,\n",
       " 'jackeroo': 77,\n",
       " 'vanakkam': 218,\n",
       " 'morgenländische': 57,\n",
       " 'østbye': 54,\n",
       " '6342': 95,\n",
       " 'paradesi': 236,\n",
       " 'networked': 3742,\n",
       " 'oligochaetes': 95,\n",
       " 'dau': 829,\n",
       " 'yasui': 381,\n",
       " 'bhoomika': 148,\n",
       " 'néerlandaise': 71,\n",
       " 'rosi': 886,\n",
       " 'eveningwear': 55,\n",
       " 'krampus': 219,\n",
       " 'flattened': 9985,\n",
       " \"nara's\": 71,\n",
       " 'maniraptorans': 84,\n",
       " 'plaški': 52,\n",
       " 'mramor': 85,\n",
       " 'warnakulasuriya': 51,\n",
       " 'mipo': 191,\n",
       " '240z': 111,\n",
       " 'taüll': 52,\n",
       " 'krzywiń': 59,\n",
       " 'α-amylase': 53,\n",
       " 'yonis': 53,\n",
       " 'saand': 55,\n",
       " 'da-hee': 80,\n",
       " 'gigounon': 73,\n",
       " 'crepaldi': 162,\n",
       " 'makarkin': 52,\n",
       " '8839': 69,\n",
       " 'weary': 3990,\n",
       " 'moot': 2905,\n",
       " 'waterbed': 135,\n",
       " 'már': 415,\n",
       " 'sagunto': 286,\n",
       " \"s'est\": 202,\n",
       " 'vierge': 710,\n",
       " 'blackfish': 331,\n",
       " 'zeisberger': 67,\n",
       " 'yong-chun': 54,\n",
       " 'clamecy': 99,\n",
       " 'bartis': 52,\n",
       " 'kesiya': 54,\n",
       " 'coffs': 852,\n",
       " 'chadian': 1303,\n",
       " 'nikolay': 8681,\n",
       " 'sub-officer': 137,\n",
       " 'carabineros': 320,\n",
       " 'myung-soo': 164,\n",
       " 'soybeans': 1997,\n",
       " 'kesteven': 997,\n",
       " 'corruption-free': 59,\n",
       " 'referential': 672,\n",
       " \"yue's\": 174,\n",
       " 'hikaye': 53,\n",
       " \"bayle's\": 51,\n",
       " 'dunlin': 494,\n",
       " 'motacilla': 484,\n",
       " 'fizzles': 138,\n",
       " 'post-premiership': 59,\n",
       " 'abkhazians': 167,\n",
       " 'gullah': 362,\n",
       " 'godward': 124,\n",
       " 'hanlan': 162,\n",
       " 'regesta': 229,\n",
       " '2005tier': 266,\n",
       " 'multistate': 178,\n",
       " 'co-planar': 66,\n",
       " 'shortest-lived': 198,\n",
       " 'valiyapally': 86,\n",
       " 'johne': 101,\n",
       " \"chatham's\": 132,\n",
       " 'pozdnyakova': 59,\n",
       " 'ojala': 235,\n",
       " 'restrung': 80,\n",
       " 'gspc': 95,\n",
       " 'hominoids': 64,\n",
       " 'medini': 121,\n",
       " \"wentworth's\": 246,\n",
       " '2000-2002': 1163,\n",
       " 'snobbery': 499,\n",
       " 'cocycle': 76,\n",
       " 'raisi': 147,\n",
       " 'jenners': 79,\n",
       " 'defla': 120,\n",
       " '603': 9206,\n",
       " 'kranzler': 67,\n",
       " 'hazlett': 337,\n",
       " 'blurting': 88,\n",
       " 'small-business': 284,\n",
       " 'tsm': 287,\n",
       " 'wsl': 1059,\n",
       " \"merchant's\": 1628,\n",
       " 'gooden': 852,\n",
       " 'earth-filled': 342,\n",
       " 'maidenhair': 206,\n",
       " 'bruselas': 61,\n",
       " 'moune': 63,\n",
       " 'ragozin': 119,\n",
       " 'qamar-ud-din': 52,\n",
       " 'thaha': 128,\n",
       " 'won-sook': 53,\n",
       " \"tacitus's\": 105,\n",
       " 'finalscoreseedoppositions': 67,\n",
       " 'idiopidae': 55,\n",
       " 'ethnobiology': 110,\n",
       " 'yantras': 51,\n",
       " 'puréed': 127,\n",
       " 'sybaris': 156,\n",
       " 'morni': 101,\n",
       " 'stallybrass': 116,\n",
       " 'vits': 69,\n",
       " \"fo'c'sle\": 106,\n",
       " 'ayyad': 162,\n",
       " 'guantánamo': 1278,\n",
       " 'sods': 280,\n",
       " 'vouchsafe': 69,\n",
       " '2961': 203,\n",
       " '2000-2013': 131,\n",
       " 'héroes': 751,\n",
       " 'greenbrae': 122,\n",
       " 'stadium19': 186,\n",
       " 'taxonomies': 426,\n",
       " 'seabrooke': 71,\n",
       " 'coiffaiti': 55,\n",
       " 'palagi': 84,\n",
       " 'enti': 108,\n",
       " 'microtransaction': 83,\n",
       " 'badrić': 69,\n",
       " 'vigie': 70,\n",
       " 'jomtien': 61,\n",
       " \"summoner's\": 123,\n",
       " 'salvaterra': 140,\n",
       " 'altissimus': 74,\n",
       " 'korneff': 66,\n",
       " 'fahlström': 67,\n",
       " 'sogliuzzo': 93,\n",
       " 'iberian': 8342,\n",
       " 'cadets': 8797,\n",
       " 'madiot': 83,\n",
       " 'owaisi': 115,\n",
       " 'conirostris': 86,\n",
       " 'southafrica': 64,\n",
       " 'koresh': 180,\n",
       " 'cus': 410,\n",
       " 'f1m': 55,\n",
       " 'burroughes': 87,\n",
       " 'outlier': 1165,\n",
       " 'chabrier': 358,\n",
       " 'laumann': 161,\n",
       " 'peeves': 92,\n",
       " 'greenup': 462,\n",
       " 'wiskunde': 118,\n",
       " 'schönburg': 92,\n",
       " 'löns': 70,\n",
       " 'shinikova': 208,\n",
       " 'gården': 62,\n",
       " 'zacky': 91,\n",
       " 'archipedia': 85,\n",
       " 'relieving': 3884,\n",
       " 'ddp': 395,\n",
       " 'subsector': 224,\n",
       " 'muscogee': 1025,\n",
       " 'broschi': 81,\n",
       " 'mortagne': 178,\n",
       " 'organoleptic': 101,\n",
       " 'academics': 65714,\n",
       " 'veszprém': 1250,\n",
       " 'classify': 7054,\n",
       " 'gross': 35572,\n",
       " \"gaiman's\": 540,\n",
       " 'hatosy': 98,\n",
       " 'executable': 1926,\n",
       " 'butyric': 225,\n",
       " 'virginias': 92,\n",
       " 'copulatory': 459,\n",
       " 'infiltrators': 707,\n",
       " 'myotis': 685,\n",
       " 'zeller': 2892,\n",
       " 'avea': 84,\n",
       " 'fiorino': 149,\n",
       " 'apoe': 101,\n",
       " 'mahdavikia': 82,\n",
       " 'gillanders': 145,\n",
       " 'liverpool-born': 74,\n",
       " 'then-existing': 299,\n",
       " \"o'flaherty's\": 60,\n",
       " 'mención': 63,\n",
       " 'verdonck': 168,\n",
       " 'hagel': 594,\n",
       " 'kapisi': 87,\n",
       " '5011': 349,\n",
       " 'ershad': 758,\n",
       " 'gandi': 218,\n",
       " 'minasyan': 146,\n",
       " 'privato': 89,\n",
       " 'jizo': 81,\n",
       " 'navicular': 173,\n",
       " 'rscm': 58,\n",
       " '11119': 52,\n",
       " 'broncs': 320,\n",
       " 'mcgahey': 178,\n",
       " 'stefanelli': 209,\n",
       " 'counter-guerrilla': 178,\n",
       " 'kamatari': 128,\n",
       " 'bitter-tasting': 93,\n",
       " 'polgár': 274,\n",
       " 'snail': 29621,\n",
       " 'tappahannock': 128,\n",
       " 'hands-on': 5111,\n",
       " 'nicholaw': 85,\n",
       " 'yarrell': 61,\n",
       " 'post-doc': 382,\n",
       " 'triangulatus': 63,\n",
       " 'vrijdag': 51,\n",
       " 'crotaphytus': 60,\n",
       " 'subordinate': 16449,\n",
       " 'tutone': 74,\n",
       " 'geshe': 276,\n",
       " 'arkadin': 98,\n",
       " 'bindery': 241,\n",
       " 'dazzy': 131,\n",
       " 'potamkin': 88,\n",
       " 'yalnız': 85,\n",
       " 'montsouris': 81,\n",
       " 'napred': 63,\n",
       " 'sult': 198,\n",
       " 'ikl': 85,\n",
       " \"package's\": 100,\n",
       " 'katsuma': 62,\n",
       " 'prevert': 57,\n",
       " '30-points': 54,\n",
       " 'ritsema': 191,\n",
       " 'record1lnovember': 52,\n",
       " 'donaldtrump': 57,\n",
       " 'yeardivision20x20px': 58,\n",
       " 'ppu': 172,\n",
       " 'bratwurst': 232,\n",
       " 'maylene': 108,\n",
       " 'threepence': 145,\n",
       " 'lareau': 262,\n",
       " 'sciences': 128194,\n",
       " 'sade': 2323,\n",
       " 'heath': 26190,\n",
       " 'intima': 192,\n",
       " 'raritan': 1282,\n",
       " 'disquisitions': 105,\n",
       " 'layali': 90,\n",
       " 'regurgitation': 507,\n",
       " 'villarroel': 448,\n",
       " 'escrick': 119,\n",
       " 'nirodha': 57,\n",
       " 'evol': 491,\n",
       " '1930-1945': 113,\n",
       " 'power-operated': 233,\n",
       " 'sydneysydney': 59,\n",
       " 'yapım': 82,\n",
       " 'zehdenick': 63,\n",
       " 'shyamalan': 372,\n",
       " 'خالد': 112,\n",
       " 'ballybrack': 83,\n",
       " '19825': 83,\n",
       " 'tram-trains': 77,\n",
       " 'ucicky': 122,\n",
       " 'adjourned': 2571,\n",
       " 'séraphin': 253,\n",
       " 'ceq': 79,\n",
       " \"cavanagh's\": 60,\n",
       " 'primates': 4729,\n",
       " 'hor': 961,\n",
       " 'mandate': 25424,\n",
       " 'montjeu': 124,\n",
       " 'magisterial': 979,\n",
       " 'toles': 160,\n",
       " 'peacham': 125,\n",
       " 'mayapur': 82,\n",
       " 'blomdahl': 98,\n",
       " 'mendenhall': 943,\n",
       " 'horseshoe': 6865,\n",
       " 'turd': 236,\n",
       " 'pittoni': 137,\n",
       " 'kaji': 957,\n",
       " 'masaki': 2433,\n",
       " '100million': 156,\n",
       " 'championshipsaddis': 137,\n",
       " 'karanka': 117,\n",
       " 'санкт-петербургского': 104,\n",
       " 'cryptosporidiosis': 100,\n",
       " 'savinkov': 80,\n",
       " 'jixian': 126,\n",
       " 'nacon': 80,\n",
       " 'longeron': 168,\n",
       " 'małogoszcz': 69,\n",
       " 'roblès': 68,\n",
       " 'headward': 64,\n",
       " 'vendt': 66,\n",
       " 'mandamentos': 58,\n",
       " 'oikya': 89,\n",
       " 'haddou': 75,\n",
       " 'ingabire': 51,\n",
       " 'electionhouseconstituency': 57,\n",
       " 'alamosa-class': 70,\n",
       " 'blackboard': 1414,\n",
       " \"racing's\": 1509,\n",
       " 'w-cdma': 94,\n",
       " 'clog': 946,\n",
       " 'thermoregulatory': 174,\n",
       " 'fellenberg': 52,\n",
       " 'reiff': 369,\n",
       " 'epermeniidae': 237,\n",
       " 'circumlocution': 96,\n",
       " 'ewelme': 129,\n",
       " 'santiago-hudson': 129,\n",
       " 'lajwanti': 89,\n",
       " 'sakimoto': 149,\n",
       " 'merry-go-rounds': 67,\n",
       " 'oldfield': 3157,\n",
       " 'norin': 141,\n",
       " 'treating': 18951,\n",
       " 'kurtosis': 144,\n",
       " 'huddle': 1028,\n",
       " \"buzzfeed's\": 111,\n",
       " 'mmediterranean': 146,\n",
       " 'stena': 576,\n",
       " 'pinetop': 304,\n",
       " 'mutable': 444,\n",
       " 'subventions': 90,\n",
       " 'jehovah': 789,\n",
       " 'glassed-in': 100,\n",
       " 'anamika': 419,\n",
       " 'toter': 82,\n",
       " 'balma': 276,\n",
       " 'krycek': 66,\n",
       " 'non-repudiation': 84,\n",
       " 'detik': 75,\n",
       " 'stoneywood': 69,\n",
       " 'cyornis': 106,\n",
       " 'casser': 93,\n",
       " 'hoynes': 70,\n",
       " 'cab-forward': 52,\n",
       " '37-0': 94,\n",
       " 'pisacane': 97,\n",
       " \"portman's\": 149,\n",
       " 'p450s': 53,\n",
       " 'prm': 472,\n",
       " 'despacito': 426,\n",
       " 'apmc': 92,\n",
       " 'jamaica': 28023,\n",
       " 'mid-engined': 517,\n",
       " 'necnon': 73,\n",
       " '265th': 225,\n",
       " 'ridonculous': 79,\n",
       " 'traitement': 224,\n",
       " 'tagala': 84,\n",
       " 'amoskeag': 129,\n",
       " 'chakraborti': 63,\n",
       " 'irish-based': 163,\n",
       " '0891': 103,\n",
       " 'bhagnani': 91,\n",
       " '8065': 89,\n",
       " 'choung': 63,\n",
       " 'edmonson': 559,\n",
       " 'pizhou': 60,\n",
       " 'lachenal': 101,\n",
       " 'kubík': 89,\n",
       " 'vennard': 54,\n",
       " 'kaiser-titz': 90,\n",
       " 'barcena': 56,\n",
       " 'phaserounddraw': 143,\n",
       " 'uçk': 53,\n",
       " 'budiša': 58,\n",
       " 'ust-kubinsky': 182,\n",
       " 'parian': 309,\n",
       " '4467': 81,\n",
       " 'minora': 282,\n",
       " 'dutilleux': 346,\n",
       " 'dodrill': 58,\n",
       " 'twala': 117,\n",
       " 'vineland': 691,\n",
       " 'pc-98': 156,\n",
       " \"powell's\": 2097,\n",
       " 'takers': 1798,\n",
       " 'wikitable': 8647,\n",
       " 'kayastha': 383,\n",
       " '282nd': 190,\n",
       " 'earwig': 390,\n",
       " 'imperiously': 62,\n",
       " 'avarice': 748,\n",
       " 'doen': 213,\n",
       " 'swatting': 172,\n",
       " 'arguable': 644,\n",
       " 'nikel': 115,\n",
       " 'griga': 98,\n",
       " 'sonorama': 127,\n",
       " 'beres': 349,\n",
       " 'traceroute': 85,\n",
       " '3rd9': 71,\n",
       " 'lumad': 175,\n",
       " 'crees': 212,\n",
       " 'juster': 225,\n",
       " 'capitolini': 166,\n",
       " '아름다운': 113,\n",
       " 'e-meter': 56,\n",
       " 'chanh': 129,\n",
       " 'nha23': 100,\n",
       " 'glamourous': 68,\n",
       " '73first': 86,\n",
       " 'kerhonkson': 63,\n",
       " '14-11': 267,\n",
       " 'staveren': 66,\n",
       " 'guohua': 199,\n",
       " 'pegler': 360,\n",
       " 'enriching': 1883,\n",
       " 'ploceus': 264,\n",
       " 'mallari': 177,\n",
       " 'geetham': 320,\n",
       " 'kreutzberger': 55,\n",
       " 'evendale': 61,\n",
       " 'turista': 120,\n",
       " 'turko': 72,\n",
       " 'tersana': 131,\n",
       " 'claude': 35458,\n",
       " 'houten': 924,\n",
       " 'stadium16': 299,\n",
       " 'yohannes': 438,\n",
       " 'neusa': 81,\n",
       " 'peiken': 135,\n",
       " 'suling': 125,\n",
       " 'nipun': 145,\n",
       " 'millilitres': 117,\n",
       " 'mocsáry': 66,\n",
       " 'raymart': 81,\n",
       " 'egoi': 61,\n",
       " 'rasulid': 70,\n",
       " 'boyarin': 100,\n",
       " 'unra': 53,\n",
       " 'cantharidae': 208,\n",
       " '2009-2018': 182,\n",
       " 'picnicking': 1802,\n",
       " '1972-1975': 390,\n",
       " 'veran': 78,\n",
       " 'kanichi': 87,\n",
       " \"perrot's\": 81,\n",
       " 'williamsbridge': 126,\n",
       " 'cortex': 5397,\n",
       " 'murga': 266,\n",
       " 'viability': 6659,\n",
       " 'frankie': 15616,\n",
       " 'wwf': 4580,\n",
       " 'pertz': 108,\n",
       " 'middleton': 10962,\n",
       " 'mirella': 622,\n",
       " 'spooked': 584,\n",
       " 'werdenberg': 111,\n",
       " 'prosser': 1502,\n",
       " 'wellpoint': 55,\n",
       " 'pozo': 1184,\n",
       " 'tracht': 102,\n",
       " 'root-mean-square': 80,\n",
       " 'generi': 83,\n",
       " 'guaitecas': 70,\n",
       " 'pérouse': 301,\n",
       " 'rette': 130,\n",
       " 'kulothunga': 236,\n",
       " 'decemvirs': 86,\n",
       " 'as365': 121,\n",
       " 'rhe': 120,\n",
       " '0718': 113,\n",
       " '4922': 155,\n",
       " '21994': 139,\n",
       " 'tiznit': 89,\n",
       " \"kuroko's\": 153,\n",
       " 'orvil': 128,\n",
       " 'ptahhotep': 51,\n",
       " \"bai's\": 180,\n",
       " 'ellender': 187,\n",
       " 'penalty-kick': 72,\n",
       " 'expectation-maximization': 60,\n",
       " 'stripling': 309,\n",
       " 'navegantes': 908,\n",
       " '#43': 1566,\n",
       " 'resuscitated': 665,\n",
       " 'pachi': 163,\n",
       " 'prabowo': 218,\n",
       " \"bootsy's\": 102,\n",
       " 'iugs': 88,\n",
       " 'mov': 605,\n",
       " \"rasputin's\": 139,\n",
       " 'uku': 276,\n",
       " 'ypbpr': 56,\n",
       " 'o-type': 210,\n",
       " 'fonio': 83,\n",
       " 'eternitatea': 53,\n",
       " 'flaxen': 155,\n",
       " 'james-collier': 92,\n",
       " 'nobilis': 1296,\n",
       " 'bechke': 52,\n",
       " 'gvozdenović': 117,\n",
       " 'arch-nemesis': 155,\n",
       " 'dinosaur-bearing': 669,\n",
       " 'joda': 123,\n",
       " '2017june': 108,\n",
       " 'catepanate': 80,\n",
       " 'laarni': 62,\n",
       " '0483': 72,\n",
       " 'hairballs': 54,\n",
       " 'namorados': 66,\n",
       " 'vice-legate': 66,\n",
       " 'fulgoromorpha': 62,\n",
       " 'iam8bit': 56,\n",
       " 'jailhouse': 1271,\n",
       " 'plasters': 298,\n",
       " 'sub-organist': 65,\n",
       " 'shannara': 225,\n",
       " '23-20': 200,\n",
       " 'tyga': 864,\n",
       " 'putrid': 415,\n",
       " \"s4c's\": 117,\n",
       " 'swiftsure': 274,\n",
       " 'erhart': 209,\n",
       " 'moyland': 67,\n",
       " 'empain': 105,\n",
       " 'pell': 2416,\n",
       " 'groot': 2483,\n",
       " 'naturelle': 2950,\n",
       " 'sub-caste': 301,\n",
       " 'carats': 485,\n",
       " 'vikernes': 126,\n",
       " 'šiškauskas': 69,\n",
       " \"che's\": 118,\n",
       " 'vigneswaran': 94,\n",
       " 'reigate': 1267,\n",
       " 'shamefully': 290,\n",
       " 'yakimenko': 76,\n",
       " 'ptyonoprogne': 160,\n",
       " 'sampaguita': 370,\n",
       " 'regularizing': 99,\n",
       " 'ep-3e': 55,\n",
       " 'scientism': 231,\n",
       " 'haig-brown': 85,\n",
       " 'jagatjit': 69,\n",
       " 'westaway': 231,\n",
       " 'apiculatus': 90,\n",
       " 'bugbear': 123,\n",
       " 'collegedale': 76,\n",
       " 'seethai': 152,\n",
       " 'meiwa': 128,\n",
       " 'chorwerke': 62,\n",
       " 'destroying': 28852,\n",
       " \"l'altro\": 119,\n",
       " 'remee': 116,\n",
       " 'manifolds': 2346,\n",
       " 'geodesic': 1302,\n",
       " 'nørrebro': 338,\n",
       " 'starrs': 175,\n",
       " \"willa's\": 81,\n",
       " 'duchessa': 117,\n",
       " 'chattahoochee-oconee': 105,\n",
       " ...}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_body.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f09a53d",
   "metadata": {},
   "source": [
    "OM TEST : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdaba44c",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "unpickling stack underflow",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnpicklingError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m tit_index\u001B[38;5;241m=\u001B[39m\u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_pickle\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/home/dataproc/tit_index.pkl\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/pandas/io/pickle.py:203\u001B[0m, in \u001B[0;36mread_pickle\u001B[0;34m(filepath_or_buffer, compression, storage_options)\u001B[0m\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m warnings\u001B[38;5;241m.\u001B[39mcatch_warnings(record\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;66;03m# We want to silence any warnings about, e.g. moved modules.\u001B[39;00m\n\u001B[1;32m    202\u001B[0m         warnings\u001B[38;5;241m.\u001B[39msimplefilter(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;167;01mWarning\u001B[39;00m)\n\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandles\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m excs_to_catch:\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;66;03m# e.g.\u001B[39;00m\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001B[39;00m\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001B[39;00m\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pc\u001B[38;5;241m.\u001B[39mload(handles\u001B[38;5;241m.\u001B[39mhandle, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[0;31mUnpicklingError\u001B[0m: unpickling stack underflow"
     ]
    }
   ],
   "source": [
    "tit_index=pd.read_pickle('/home/dataproc/tit_index.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf6640",
   "metadata": {},
   "source": [
    "## DL OF BODY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11261dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DL(text,id):\n",
    "  lst_words=[]\n",
    "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "  for i in tokens:\n",
    "    if i not in all_stopwords:\n",
    "      lst_words.append(i)\n",
    "  \n",
    "  return ((id,len(lst_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9376e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_text_DL = parquetFile.select(\"text\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de5b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "doc2len = doc_text_DL.map(lambda x:DL(x[0],x[1]))\n",
    "dict_DL = doc2len.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "431392ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Create a dictionary\n",
    "my_dict = {\"a\": 1, \"b\": 2, \"c\": 3}\n",
    "\n",
    "# Open a file to write the pickle to\n",
    "with open(\"dict_DL.pkl\", \"wb\") as pickle_file:\n",
    "    # Write the dictionary to the file\n",
    "    pickle.dump(dict_DL, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabec5e0",
   "metadata": {},
   "source": [
    "## Anchor Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d6a7fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_text_pairs_anchor_text=parquetFile.select(\"anchor_text\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a418af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2anchor=doc_text_pairs_anchor_text.flatMap(lambda x: x[0])\n",
    "id2text=id2anchor.distinct().groupByKey().mapValues(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7eafa576",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = id2text.flatMap(lambda x: word_count(x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51d5cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf13be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "w2df_d = calculate_df(postings)\n",
    "w2df_anchor = w2df_d.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5265bede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "w2termanchor = calculate_term_total(postings)\n",
    "dict_term_total_anchor = w2termanchor.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52a136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "_ = partition_postings_and_write(postings).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "badcabb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_posting_locs_ANCOR = defaultdict(list)\n",
    "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "  if not blob.name.endswith(\"pickle\"):\n",
    "    continue\n",
    "  with blob.open(\"rb\") as f:\n",
    "    posting_locs = pickle.load(f)\n",
    "    for k, v in posting_locs.items():\n",
    "      super_posting_locs_ANCOR[k].extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a825f84e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH\n"
     ]
    }
   ],
   "source": [
    "inverted_index_anchor = InvertedIndex()\n",
    "# Adding the posting locations dictionary to the inverted index\n",
    "inverted_index_anchor.posting_locs = super_posting_locs_ANCOR\n",
    "# Add the token - df dictionary to the inverted index\n",
    "inverted_index_anchor.df = w2df_anchor\n",
    "#Add the total terms for each term\n",
    "inverted_index_anchor.term_total = dict_term_total_anchor\n",
    "# write the global stats out\n",
    "\n",
    "inverted_index_anchor.write_index('.','anchor_index')\n",
    "print('FINISH')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}